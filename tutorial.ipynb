{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# \"Machine learning\" with text (in Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unicodedata  # mekcene dlzne a divne charaktery\n",
    "import re # praca s textom (regexy)\n",
    "import csv # comma separated values\n",
    "import json # zoberie json subor a vyrobi data\n",
    "import numpy as np\n",
    "unimported = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for Python 2 users\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction to supervised learning in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**From <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\">Wikipedia</a>:**<br>\n",
    "**Supervised learning** is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value.\n",
    "\n",
    "**Note:** We will consider a classification task, i.e., samples belong to two or more classes that we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets load the data. If you had sklearn, we would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "# Load the iris dataset.\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However we have iris dataset in data folder. Colums are: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iris = np.array(json.load(open('omg.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"we will get rid of class 2 for conveniece\"\"\"\n",
    "iris = iris[iris[:,-1]!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the shapes of X and y.\n",
    "print(iris.shape)\n",
    "n_features = iris.shape[1]\n",
    "n_features-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2,  0. ],\n",
       "       [ 4.9,  3. ,  1.4,  0.2,  0. ],\n",
       "       [ 4.7,  3.2,  1.3,  0.2,  0. ],\n",
       "       [ 4.6,  3.1,  1.5,  0.2,  0. ],\n",
       "       [ 5. ,  3.6,  1.4,  0.2,  0. ],\n",
       "       [ 5.4,  3.9,  1.7,  0.4,  0. ],\n",
       "       [ 4.6,  3.4,  1.4,  0.3,  0. ],\n",
       "       [ 5. ,  3.4,  1.5,  0.2,  0. ],\n",
       "       [ 4.4,  2.9,  1.4,  0.2,  0. ],\n",
       "       [ 4.9,  3.1,  1.5,  0.1,  0. ],\n",
       "       [ 5.4,  3.7,  1.5,  0.2,  0. ],\n",
       "       [ 4.8,  3.4,  1.6,  0.2,  0. ],\n",
       "       [ 4.8,  3. ,  1.4,  0.1,  0. ],\n",
       "       [ 4.3,  3. ,  1.1,  0.1,  0. ],\n",
       "       [ 5.8,  4. ,  1.2,  0.2,  0. ],\n",
       "       [ 5.7,  4.4,  1.5,  0.4,  0. ],\n",
       "       [ 5.4,  3.9,  1.3,  0.4,  0. ],\n",
       "       [ 5.1,  3.5,  1.4,  0.3,  0. ],\n",
       "       [ 5.7,  3.8,  1.7,  0.3,  0. ],\n",
       "       [ 5.1,  3.8,  1.5,  0.3,  0. ],\n",
       "       [ 5.4,  3.4,  1.7,  0.2,  0. ],\n",
       "       [ 5.1,  3.7,  1.5,  0.4,  0. ],\n",
       "       [ 4.6,  3.6,  1. ,  0.2,  0. ],\n",
       "       [ 5.1,  3.3,  1.7,  0.5,  0. ],\n",
       "       [ 4.8,  3.4,  1.9,  0.2,  0. ],\n",
       "       [ 5. ,  3. ,  1.6,  0.2,  0. ],\n",
       "       [ 5. ,  3.4,  1.6,  0.4,  0. ],\n",
       "       [ 5.2,  3.5,  1.5,  0.2,  0. ],\n",
       "       [ 5.2,  3.4,  1.4,  0.2,  0. ],\n",
       "       [ 4.7,  3.2,  1.6,  0.2,  0. ],\n",
       "       [ 4.8,  3.1,  1.6,  0.2,  0. ],\n",
       "       [ 5.4,  3.4,  1.5,  0.4,  0. ],\n",
       "       [ 5.2,  4.1,  1.5,  0.1,  0. ],\n",
       "       [ 5.5,  4.2,  1.4,  0.2,  0. ],\n",
       "       [ 4.9,  3.1,  1.5,  0.1,  0. ],\n",
       "       [ 5. ,  3.2,  1.2,  0.2,  0. ],\n",
       "       [ 5.5,  3.5,  1.3,  0.2,  0. ],\n",
       "       [ 4.9,  3.1,  1.5,  0.1,  0. ],\n",
       "       [ 4.4,  3. ,  1.3,  0.2,  0. ],\n",
       "       [ 5.1,  3.4,  1.5,  0.2,  0. ],\n",
       "       [ 5. ,  3.5,  1.3,  0.3,  0. ],\n",
       "       [ 4.5,  2.3,  1.3,  0.3,  0. ],\n",
       "       [ 4.4,  3.2,  1.3,  0.2,  0. ],\n",
       "       [ 5. ,  3.5,  1.6,  0.6,  0. ],\n",
       "       [ 5.1,  3.8,  1.9,  0.4,  0. ],\n",
       "       [ 4.8,  3. ,  1.4,  0.3,  0. ],\n",
       "       [ 5.1,  3.8,  1.6,  0.2,  0. ],\n",
       "       [ 4.6,  3.2,  1.4,  0.2,  0. ],\n",
       "       [ 5.3,  3.7,  1.5,  0.2,  0. ],\n",
       "       [ 5. ,  3.3,  1.4,  0.2,  0. ],\n",
       "       [ 7. ,  3.2,  4.7,  1.4,  1. ],\n",
       "       [ 6.4,  3.2,  4.5,  1.5,  1. ],\n",
       "       [ 6.9,  3.1,  4.9,  1.5,  1. ],\n",
       "       [ 5.5,  2.3,  4. ,  1.3,  1. ],\n",
       "       [ 6.5,  2.8,  4.6,  1.5,  1. ],\n",
       "       [ 5.7,  2.8,  4.5,  1.3,  1. ],\n",
       "       [ 6.3,  3.3,  4.7,  1.6,  1. ],\n",
       "       [ 4.9,  2.4,  3.3,  1. ,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3,  1. ],\n",
       "       [ 5.2,  2.7,  3.9,  1.4,  1. ],\n",
       "       [ 5. ,  2. ,  3.5,  1. ,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5,  1. ],\n",
       "       [ 6. ,  2.2,  4. ,  1. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4,  1. ],\n",
       "       [ 5.6,  2.9,  3.6,  1.3,  1. ],\n",
       "       [ 6.7,  3.1,  4.4,  1.4,  1. ],\n",
       "       [ 5.6,  3. ,  4.5,  1.5,  1. ],\n",
       "       [ 5.8,  2.7,  4.1,  1. ,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5,  1. ],\n",
       "       [ 5.6,  2.5,  3.9,  1.1,  1. ],\n",
       "       [ 5.9,  3.2,  4.8,  1.8,  1. ],\n",
       "       [ 6.1,  2.8,  4. ,  1.3,  1. ],\n",
       "       [ 6.3,  2.5,  4.9,  1.5,  1. ],\n",
       "       [ 6.1,  2.8,  4.7,  1.2,  1. ],\n",
       "       [ 6.4,  2.9,  4.3,  1.3,  1. ],\n",
       "       [ 6.6,  3. ,  4.4,  1.4,  1. ],\n",
       "       [ 6.8,  2.8,  4.8,  1.4,  1. ],\n",
       "       [ 6.7,  3. ,  5. ,  1.7,  1. ],\n",
       "       [ 6. ,  2.9,  4.5,  1.5,  1. ],\n",
       "       [ 5.7,  2.6,  3.5,  1. ,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1,  1. ],\n",
       "       [ 5.5,  2.4,  3.7,  1. ,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2,  1. ],\n",
       "       [ 6. ,  2.7,  5.1,  1.6,  1. ],\n",
       "       [ 5.4,  3. ,  4.5,  1.5,  1. ],\n",
       "       [ 6. ,  3.4,  4.5,  1.6,  1. ],\n",
       "       [ 6.7,  3.1,  4.7,  1.5,  1. ],\n",
       "       [ 6.3,  2.3,  4.4,  1.3,  1. ],\n",
       "       [ 5.6,  3. ,  4.1,  1.3,  1. ],\n",
       "       [ 5.5,  2.5,  4. ,  1.3,  1. ],\n",
       "       [ 5.5,  2.6,  4.4,  1.2,  1. ],\n",
       "       [ 6.1,  3. ,  4.6,  1.4,  1. ],\n",
       "       [ 5.8,  2.6,  4. ,  1.2,  1. ],\n",
       "       [ 5. ,  2.3,  3.3,  1. ,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3,  1. ],\n",
       "       [ 5.7,  3. ,  4.2,  1.2,  1. ],\n",
       "       [ 5.7,  2.9,  4.2,  1.3,  1. ],\n",
       "       [ 6.2,  2.9,  4.3,  1.3,  1. ],\n",
       "       [ 5.1,  2.5,  3. ,  1.1,  1. ],\n",
       "       [ 5.7,  2.8,  4.1,  1.3,  1. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets examin features even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.471  3.094  2.862  0.785  0.5  ]\n",
      "[ 0.63848179  0.47367077  1.44130358  0.5634492   0.5       ]\n"
     ]
    }
   ],
   "source": [
    "# Compute basic statistics for data : count each label, count mean + std of each feature.\n",
    "\n",
    "\"\"\" <<CODE>>\"\"\"\n",
    "ind = np.bincount( iris[:,-1].astype(int) ) # pocet kategorii (nul, jednotiek)\n",
    "np.mean(iris[0:ind[0],0]) # priemer prveho stlpca s classou 0\n",
    "print( iris.mean(axis=0) ) # priemery  vsetkych stlpcov\n",
    "print( iris.std(axis=0) ) # standardne odchylky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Now, lets do some classification. \n",
    "We split data to training and testing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4247)\n",
    "np.random.shuffle(iris)\n",
    "train_count = int(iris.shape[0]*0.9)\n",
    "train = iris[:train_count]\n",
    "train_x = train[:,:-1]\n",
    "train_y = train[:,-1]\n",
    "test = iris[train_count:]\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 38]\n",
      "[ 5.4525   3.10625  2.8075   0.7675 ]\n",
      "[ 5.545  3.045  3.08   0.855]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Do we have good test set (are labels balanced?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[ 5.26  3.04  2.84  0.82]\n",
      "[ 5.48210526  3.09684211  2.86315789  0.78315789]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" <<CODE>> \"\"\"\n",
    "print( np.bincount( train[:,-1].astype(int) ) ) \n",
    "print( train_x.mean(axis=0) )\n",
    "print( test_x.mean(axis=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we want to train some model, that learns how to predict last collum based on the first four.\n",
    "We will test it on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "# Init logistic regression model with default params.\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the model. \n",
    "clf.fit(train_x, train_y)\n",
    "predicted = clf.predict(test_x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How could we evaluate performance of our model? What do ve care about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def precision(predicted_y, true_y):\n",
    "    return float((predicted_y == true_y).sum())/predicted_y.size\n",
    "\n",
    "def L2(predicted_y, true_y):\n",
    "    return 0.5 * ((predicted_y-true_y)**2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=-3\n",
    "np.absolute(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "But first, lets make some benchmarks. What happends if we predict alway only one value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train model, that predicts only one value and evaluate its performance on test set\n",
    "\"\"\"<<CODE>>\"\"\"\n",
    "\n",
    "def klasifikator(new, train, neigbours):\n",
    "    length = train.shape[0]\n",
    "    dist = np.zeros((length))\n",
    "    for i in range(length):\n",
    "        dist[i] = np.absolute( new[0] - iris[i,0] )\n",
    "    return iris[dist.argmin(),-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "length = test.shape[0]\n",
    "chyba = 0\n",
    "for i in range(length):\n",
    "    chyba += L2(klasifikator(test_x[i], train, 1), test_y[i])\n",
    "print(chyba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nearest neighbour classifier\n",
    "Classify based on the nearest example in training set. Distance between examples is euklidean. Try cosine distance as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"<<CODE>>\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Try to consider 10 nearest samples. Is it better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<CODE>>'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"<<CODE>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Any potential problems?  Are features considered equally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logistic regression\n",
    "Our model will be a simple logistic regression.\n",
    "\n",
    "We want to predict $\\hat{y}$ with a following formula $\\hat{y}= h(wx + b)$, where $h(x)$ is some form of nonlinearity, often sigmoid $h(x) = \\frac{1}{1-e^x}$.\n",
    "\n",
    "We wanto to find such $w$ and such $b$ that minimize the $L = \\frac{1}{2}(\\hat{y}-y)^2$. \n",
    "\n",
    "Iteratively (for a few times) update $b$ and $w$ with a $b -= \\alpha \\frac{\\partial L}{\\partial b}$, $w -= \\alpha \\frac{\\partial L}{\\partial w}$. We can do it for one example at a time, or for all examples at onse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set some alpha\n",
    "# initiazet b and w on some small, positive values.\n",
    "\n",
    "for epoch in range(100):\n",
    "    # compute Los\n",
    "    # update w and b\n",
    "    # print Los\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data normalization\n",
    "Remember, are all features treated equaly? \n",
    "Set feature means to $0$ and standard deviations to $1$ and run above methods again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\" <<CODE>> \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What about text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_dataset = [\"A coward judges all he sees by what he is.\",\n",
    "                \"There are people who need people to need them.\",\n",
    "                \"Never's the word God listens for when he needs a laugh.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can not feed text to linear regression :(."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to transform it to numbers. Each dimension is one word in wocabulary. We will ignore words that appear only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "# Init CountVectorizer with the default params.\n",
    "vectorizer = CountVectorizer()\n",
    "# Learn the vocabulary from the text data.\n",
    "vectorizer.fit(text_dataset)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count each word in a dataset and encode it into \"bag of words\".\n",
    "\"\"\"<<CODE>>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alza sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "alza = json.load(open('data/alza.json'))\n",
    "train_count = int(len(alza)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alza[]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ratings = [x['rating'] for x in alza]\n",
    "texts = [x['text'] for x in alza]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Since we have texts written in Czech in the dataset, let's remove the accents (diacritics) from the text first.\n",
    "# This may remove come information, but you wont get an univode error.\n",
    "def remove_accents(s):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', s)\n",
    "    ascii_string = nkfd_form.encode('ASCII', 'ignore')\n",
    "    return ascii_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'o', 'i', 't', 'a']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in \"pocitac\" if x!=\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chladi brutalne oproti original chladicu ktory bol pribaleny k jadrovemu AMD FX je tichsi no stale ho je trochu pocut nic strasne to ale neni aj pri vyzsom zatazovani sa drzi pod  stupnov co je uplna parada K instalacii ako som cital ze to moze byt zlozitejsie a tak podobne tak to je uplna primitivnost s instalaciou na socket AM neboli absolutne ziadne problemy hoci obrazkovy navod nestal za moc ak ale mate IQ aspon  verim ze vam bude bohate stacit v skratke pre tych co sa boja a nechcu ist do toho na slepo kratky navod pre socket AM treba dat dole ventilator co sa spravi miernym zapacenim uchytov  srobikmi prichytime uchyty ale nezatahujeme nechame ich na volno aby sa s nimi lahko manipulovalo nasadime na procesor a uchyty nasadime na socket pred nasadenim odporucam do dosky zapojit ventilator aby ste nemali neskor problem sa k tomu konektoru dostat a ventilator polozime na stranu aby zatial nezavadzal  dotiahneme srobiky nasadime ventilator na telo chladica hotovo ja som ho mal namontovany za asi  minut a to som sa s tym niak nehnal a ani taketo veci nerobim kazdy den'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.ascii_letters\n",
    "\"a\" in string.ascii_letters\n",
    "\"\".join([x for x in texts[0] if x in string.ascii_letters + \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chladi brutalne, oproti original chladicu, ktory bol pribaleny k 4-jadrovemu AMD FX je tichsi, no stale ho je trochu pocut (nic strasne to ale neni). aj pri vyzsom zatazovani sa drzi pod 45 stupnov, co je uplna parada. K instalacii, ako som cital, ze to moze byt zlozitejsie a tak podobne, tak to je uplna primitivnost, s instalaciou (na socket AM3+) neboli absolutne ziadne problemy, hoci obrazkovy navod nestal za moc, ak ale mate IQ aspon 80, verim, ze vam bude bohate stacit. -v skratke pre tych, co sa boja a nechcu ist do toho na slepo. kratky navod (pre socket AM3+): 1.treba dat dole ventilator, co sa spravi miernym zapacenim uchytov 2. srobikmi prichytime uchyty, ale nezatahujeme, nechame ich na volno, aby sa s nimi lahko manipulovalo 3.nasadime na procesor a uchyty nasadime na socket. (pred nasadenim odporucam do dosky zapojit ventilator, aby ste nemali neskor problem sa k tomu konektoru dostat a ventilator polozime na stranu, aby zatial nezavadzal) 4. dotiahneme srobiky 5.nasadime ventilator na telo chladica -hotovo. ja som ho mal namontovany za asi 5 minut a to som sa s tym niak nehnal a ani taketo veci nerobim kazdy den.'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tokenize text and remove strange characters, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'popici'\n"
     ]
    }
   ],
   "source": [
    "print(remove_accents(\"popiƒçi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "normalize() argument 2 must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-312-70863778aa33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"<<CODE>>\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_rem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-291-a11b83bfd023>\u001b[0m in \u001b[0;36mremove_accents\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This may remove come information, but you wont get an univode error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnkfd_form\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NFKD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mascii_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnkfd_form\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ASCII'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mascii_string\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: normalize() argument 2 must be str, not list"
     ]
    }
   ],
   "source": [
    "\"\"\"<<CODE>>\"\"\"\n",
    "text_rem = remove_accents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now is time to split to train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_text = texts[:train_count]\n",
    "train_rating = ratings[train_count:]\n",
    "test_text = texts[:train_count]\n",
    "test_rating = ratings[train_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Vectorize text. Be aware that this is part of prediction! We need to use train set for training this vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<CODE>>'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform text to numeric values\n",
    "\"\"\"<<CODE>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "# Initialize the CountVectorizer, this time with customized params.\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             ngram_range=(1,3),\n",
    "                             stop_words=list(stopwords[\"word\"].values),\n",
    "                             max_df = 0.5,\n",
    "                             min_df = 30,\n",
    "                             tokenizer = lambda x: re.split(\"[\\r\\t\\n .,;:'\\\"()?!/]+\", x))\n",
    "# Learn the vocabulary and check its size.\n",
    "vectorizer.fit(text)\n",
    "len(vectorizer.get_feature_names())\n",
    "# Transform train data into a document-term matrix.\n",
    "X_train_dtm = vectorizer.transform(text)\n",
    "X_train_dtm\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13572"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lets classify\n",
    "\n",
    "Just do the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<CODE>>'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<<CODE>>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use better vectorization -> tf-idf transform. Can be tricky for sentiment analysis. \"dobre\" is basicaly a stop word, but it has discriminative pover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
